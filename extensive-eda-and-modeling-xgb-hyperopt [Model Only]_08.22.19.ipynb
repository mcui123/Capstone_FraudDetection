{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Standard plotly imports\n",
    "#import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "#import cufflinks\n",
    "#import cufflinks as cf\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Using plotly + cufflinks in offline mode\n",
    "init_notebook_mode(connected=True)\n",
    "#cufflinks.go_offline(connected=True)\n",
    "\n",
    "# Preprocessing, modelling and evaluating\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "## Hyperopt modules\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import gc\n",
    "# print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REducing memory\n",
    "# df_trans = reduce_mem_usage(df_trans)\n",
    "# df_id = reduce_mem_usage(df_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 434)\n",
      "(506691, 433)\n"
     ]
    }
   ],
   "source": [
    "df_trans = pd.read_csv('./train_transaction.csv')\n",
    "df_test_trans = pd.read_csv('./test_transaction.csv')\n",
    "\n",
    "df_id = pd.read_csv('./train_identity.csv')\n",
    "df_test_id = pd.read_csv('./test_identity.csv')\n",
    "\n",
    "sample_submission = pd.read_csv('./sample_submission.csv', index_col='TransactionID')\n",
    "\n",
    "df_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True, on='TransactionID')\n",
    "df_test = df_test_trans.merge(df_test_id, how='left', left_index=True, right_index=True, on='TransactionID')\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "# y_train = df_train['isFraud'].copy()\n",
    "del df_trans, df_id, df_test_trans, df_test_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reducing memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = reduce_mem_usage(df_train)\n",
    "# df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hours and Days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_encoder(df, offset = 0, colname = 'TransactionDT'):\n",
    "    days = df[colname] / (3600*24)\n",
    "    encode_days = np.floor(days - 1 + offset) % 7 #review\n",
    "    return encode_days\n",
    "\n",
    "def hour_encoder(df, colname = 'TransactionDT'):\n",
    "    hours = df_train['TransactionDT'] / 3600\n",
    "    encode_hours = np.floor(hours) % 24 #review\n",
    "    return encode_hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['weekday'] = day_encoder(df_train, offset = 0.58)\n",
    "df_test['weekday'] = day_encoder(df_test, offset=  0.58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['hours'] = hour_encoder(df_train)\n",
    "df_test['hours'] = hour_encoder(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "for f in df_train.drop('isFraud', axis=1).columns:\n",
    "    if df_train[f].dtype=='object' or df_test[f].dtype=='object': \n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(df_train[f].values) + list(df_test[f].values))\n",
    "        df_train[f] = lbl.transform(list(df_train[f].values))\n",
    "        df_test[f] = lbl.transform(list(df_test[f].values))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is normalizing \n",
    "df_train['Trans_min_mean'] = df_train['TransactionAmt'] - df_train['TransactionAmt'].mean()\n",
    "df_train['Trans_min_std'] = df_train['Trans_min_mean'] / df_train['TransactionAmt'].std()\n",
    "df_test['Trans_min_mean'] = df_test['TransactionAmt'] - df_test['TransactionAmt'].mean()\n",
    "df_test['Trans_min_std'] = df_test['Trans_min_mean'] / df_test['TransactionAmt'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['TransactionAmt_to_mean_card1'] = df_train['TransactionAmt'] / df_train.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "# df_train['TransactionAmt_to_mean_card4'] = df_train['TransactionAmt'] / df_train.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "# df_train['TransactionAmt_to_std_card1'] = df_train['TransactionAmt'] / df_train.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "# df_train['TransactionAmt_to_std_card4'] = df_train['TransactionAmt'] / df_train.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "# df_test['TransactionAmt_to_mean_card1'] = df_test['TransactionAmt'] / df_test.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "# df_test['TransactionAmt_to_mean_card4'] = df_test['TransactionAmt'] / df_test.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "# df_test['TransactionAmt_to_std_card1'] = df_test['TransactionAmt'] / df_test.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "# df_test['TransactionAmt_to_std_card4'] = df_test['TransactionAmt'] / df_test.groupby(['card4'])['TransactionAmt'].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['TransactionAmt'] = np.log(df_train['TransactionAmt'])\n",
    "df_test['TransactionAmt'] = np.log(df_test['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concating dfs to get PCA of V features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['isFraud'] = 'test'\n",
    "df = pd.concat([df_train, df_test], axis=0, sort=False )\n",
    "df = df.reset_index()\n",
    "df = df.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_change(df, cols, n_components, prefix='PCA_', rand_seed=4):\n",
    "    pca = PCA(n_components=n_components, random_state=rand_seed)\n",
    "\n",
    "    principalComponents = pca.fit_transform(df[cols])\n",
    "\n",
    "    principalDf = pd.DataFrame(principalComponents)\n",
    "\n",
    "    df.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "    principalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n",
    "\n",
    "    df = pd.concat([df, principalDf], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_v = df_train.columns[55:394]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "for col in mas_v:\n",
    "    df[col] = df[col].fillna((df[col].min() - 2))\n",
    "    df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n",
    "\n",
    "    \n",
    "df = PCA_change(df, mas_v, prefix='PCA_V_', n_components=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seting train and test back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = df[df['isFraud'] != 'test'], df[df['isFraud'] == 'test'].drop('isFraud', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 129)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seting X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.sort_values('TransactionDT').drop(['isFraud', \n",
    "                                                      'TransactionDT', \n",
    "                                                      #'Card_ID'\n",
    "                                                     ],\n",
    "                                                     axis=1)\n",
    "y_train = df_train.sort_values('TransactionDT')['isFraud'].astype(bool)\n",
    "\n",
    "X_test = df_test.sort_values('TransactionDT').drop(['TransactionDT',\n",
    "                                                    #'Card_ID'\n",
    "                                                   ], \n",
    "                                                   axis=1)\n",
    "del df_train\n",
    "df_test = df_test[[\"TransactionDT\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "# import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search = {\n",
    "#     \"verbosity\": [-5],\n",
    "#     \"num_boost_round\": [250, 750,1500],\n",
    "#     \"max_depth\": [250, 500, 750],\n",
    "#     \"learning_rate\": np.linspace(1e-3, 1,5),\n",
    "#     \"num_leaves\": [5,10,20],\n",
    "#     \"min_child_samples\": [10,30,50],\n",
    "#     \"min_child_weight\": [1e-3],\n",
    "#     \"subsample\": np.linspace(0.1,1,5),  # subsample = bagging\n",
    "# #     \"subsample_freq\": [],\n",
    "#     \"colsample_bytree\": [0.5, 0.7,1.0]  # feature fraction: \"mtry\"\n",
    "#     #\"max_delta_step\": -1,\n",
    "# #     \"reg_alpha\": [0.05],      # L1\n",
    "# #     \"reg_lambda\": [2]  # ,   # L2\n",
    "#     # \"min_split_gain\": 0.0,\n",
    "#     # \"drop_rate\": 0.1, # dart only\n",
    "#     # \"max_drop\": 50, # dart only\n",
    "#     # \"skip_drop\": 0.5, # dart only\n",
    "#     # \"uniform_drop\": False, # dart only\n",
    "#     # \"top_rate\": 0.2, # goss only\n",
    "#     # \"other_rate\": 0.1, # goss only\n",
    "#     # \"min_data_per_group\": 100,\n",
    "#     # \"max_cat_threshold\": 32,\n",
    "#     # \"cat_l2\": 10.0,\n",
    "#     # \"cat_smooth\": 10.0,\n",
    "#     # \"max_cat_to_onehot\": 4,\n",
    "#     # \"topk\": 20, # larger -> more accurate but slow\n",
    "    \n",
    "# }\n",
    "\n",
    "# ## n_jobs sets number of cores to parallelize on, set to 4 if you have 8 cores if you are getting bottlenecked\n",
    "# ## verbose defines how much information is printed to console during search\n",
    "\n",
    "# tss = TimeSeriesSplit(n_splits=7)\n",
    "# model_grid = GridSearchCV(lgb.LGBMRegressor(\n",
    "#     objective=\"binary\", metric=\"auc\", boosting_type=\"gbdt\", device_type=\"cpu\", tree_learner=\"feature\"),\n",
    "#     search, cv=tss)\n",
    "\n",
    "# model_grid.fit(X_train,y_train)\n",
    "\n",
    "# params = model_grid.best_params_\n",
    "\n",
    "# print(f'Best grid search parameters:\\n      {params}')\n",
    "# print(f'Best grid search loss:\\n         {model_grid.best_score_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the HyperOpt function with parameters space and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from xgboost import plot_importance\n",
    "# from sklearn.metrics import make_scorer\n",
    "\n",
    "# import time\n",
    "# def objective(params, FOLDS = 7):\n",
    "#     time1 = time.time()\n",
    "#     params = {\n",
    "#         'max_depth': int(params['max_depth']),\n",
    "#         'gamma': \"{:.3f}\".format(params['gamma']),\n",
    "#         'subsample': \"{:.2f}\".format(params['subsample']),\n",
    "#         'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n",
    "#         'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n",
    "#         'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n",
    "#         'num_leaves': '{:.3f}'.format(params['num_leaves']),\n",
    "#         'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "#         'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n",
    "#         'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n",
    "#         'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n",
    "#     }\n",
    "\n",
    "#     print(\"\\n############## New Run ################\")\n",
    "#     print(f\"params = {params}\")\n",
    "    \n",
    "#     count=1\n",
    "\n",
    "#     tss = TimeSeriesSplit(n_splits=FOLDS)\n",
    "#     y_preds = np.zeros(sample_submission.shape[0])\n",
    "#     y_oof = np.zeros(X_train.shape[0])\n",
    "#     score_mean = 0\n",
    "#     for tr_idx, val_idx in tss.split(X_train, y_train):\n",
    "#         clf = xgb.XGBClassifier(\n",
    "#             n_estimators=600, random_state=4, verbose=True,  \n",
    "#             **params\n",
    "#         )\n",
    "\n",
    "#         X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n",
    "#         y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "#         clf.fit(X_tr, y_tr)\n",
    "#         #y_pred_train = clf.predict_proba(X_vl)[:,1]\n",
    "#         #print(y_pred_train)\n",
    "#         score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n",
    "#         # plt.show()\n",
    "#         score_mean += score\n",
    "#         print(f'{count} CV - score: {round(score, 4)}')\n",
    "#         count += 1\n",
    "#     time2 = time.time() - time1\n",
    "#     print(f\"Total Time Run: {round(time2 / 60,2)}\")\n",
    "#     gc.collect()\n",
    "#     print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n",
    "#     del X_tr, X_vl, y_tr, y_vl, clf, score\n",
    "#     return -(score_mean / FOLDS)\n",
    "\n",
    "\n",
    "# space = {\n",
    "#     'max_depth': hp.quniform('max_depth', 7, 23, 1),\n",
    "#     'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n",
    "#     'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n",
    "#     'learning_rate': hp.uniform('learning_rate', 0.01, 0.2), \n",
    "#     'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9), \n",
    "#     'gamma': hp.uniform('gamma', 0.01, .7),\n",
    "#     'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n",
    "#     'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))), \n",
    "#     'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n",
    "#    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n",
    "#    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)   \n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set algoritm parameters\n",
    "# best = fmin(fn=objective,\n",
    "#             space=space,\n",
    "#             algo=tpe.suggest,\n",
    "#             max_evals=27)\n",
    "\n",
    "# # Print best parameters\n",
    "# best_params = space_eval(space, best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"BEST PARAMS: \", best_params)\n",
    "\n",
    "# best_params['max_depth'] = int(best_params['max_depth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning and Predicting with best Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting X test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train.fillna(-999)\n",
    "X_test = X_test.fillna(-999)\n",
    "\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "                        n_estimators = 500, \n",
    "                       n_jobs = 4, \n",
    "                       max_depth = 9, \n",
    "                       learning_rate = 0.05, \n",
    "                       sub_sample = 0.9, \n",
    "                       colsample_bytree = 0.9, \n",
    "                       missing = -999)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_preds = clf.predict_proba(X_test)[:,1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 20 Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TransactionID</th>\n",
       "      <td>4258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card1</th>\n",
       "      <td>3964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionAmt</th>\n",
       "      <td>3774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card2</th>\n",
       "      <td>3594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addr1</th>\n",
       "      <td>3096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D15</th>\n",
       "      <td>1784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C13</th>\n",
       "      <td>1777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA_V_13</th>\n",
       "      <td>1698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card5</th>\n",
       "      <td>1565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1</th>\n",
       "      <td>1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D8</th>\n",
       "      <td>1510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dist1</th>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA_V_15</th>\n",
       "      <td>1404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA_V_14</th>\n",
       "      <td>1402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_emaildomain</th>\n",
       "      <td>1398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA_V_0</th>\n",
       "      <td>1394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA_V_27</th>\n",
       "      <td>1393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2</th>\n",
       "      <td>1382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA_V_8</th>\n",
       "      <td>1378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA_V_17</th>\n",
       "      <td>1366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                score\n",
       "TransactionID    4258\n",
       "card1            3964\n",
       "TransactionAmt   3774\n",
       "card2            3594\n",
       "addr1            3096\n",
       "D15              1784\n",
       "C13              1777\n",
       "PCA_V_13         1698\n",
       "card5            1565\n",
       "C1               1530\n",
       "D8               1510\n",
       "dist1            1484\n",
       "PCA_V_15         1404\n",
       "PCA_V_14         1402\n",
       "P_emaildomain    1398\n",
       "PCA_V_0          1394\n",
       "PCA_V_27         1393\n",
       "D2               1382\n",
       "PCA_V_8          1378\n",
       "PCA_V_17         1366"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_important = clf.get_booster().get_score(importance_type=\"weight\")\n",
    "keys = list(feature_important.keys())\n",
    "values = list(feature_important.values())\n",
    "\n",
    "data = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n",
    "\n",
    "# Top 10 features\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seting y_pred to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['isFraud'] = y_preds\n",
    "sample_submission.to_csv('XGB_hypopt_model_08.22.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
